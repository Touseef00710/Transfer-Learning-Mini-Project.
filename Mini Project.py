# -*- coding: utf-8 -*-
"""Transfer Learning Mini Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19df3yCLvfrq9a1rkeCIgKTNc0qrWI4Dv

**Install required tensorflow library.**
"""

!pip install tensorflow tensorflow-datasets

"""**Import Libraries.**"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
import tensorflow_datasets as tfds

"""**Load CIFAR-10 dataset.**"""

(ds_train_full, ds_test), ds_info = tfds.load(
    'cifar10',
    split=['train', 'test'],
    shuffle_files=True,
    as_supervised=True,
    with_info=True
)

class_names = ds_info.features['label'].names

"""**Preprocessing and splitting**"""

# Preprocessing and splitting
BATCH_SIZE = 32
IMG_SIZE = (224, 224)

train_size = 0.8
train_count = int(ds_info.splits['train'].num_examples * train_size)

ds_train = ds_train_full.take(train_count)
ds_val = ds_train_full.skip(train_count)

def preprocess(image, label):
    image = tf.image.resize(image, IMG_SIZE)
    image = tf.cast(image, tf.float32) / 255.0
    return image, label

train_dataset = ds_train.map(preprocess).shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
validation_dataset = ds_val.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
test_dataset = ds_test.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

"""**Final test split from validation.**"""

val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(4 * val_batches // 5)
validation_dataset = validation_dataset.skip(4 * val_batches // 5)

"""**Load pretrained MobileNetV2.**"""

IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(
    input_shape=IMG_SHAPE,
    include_top=False,
    weights='imagenet'
)
base_model.trainable = False
preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input

"""**Custom classification head.**"""

# Custom classification head
data_augmentation = tf.keras.Sequential([
    tf.keras.layers.RandomFlip("horizontal"),
    tf.keras.layers.RandomRotation(0.2),
])

inputs = tf.keras.Input(shape=(224, 224, 3))
x = data_augmentation(inputs)
x = preprocess_input(x)
x = base_model(x, training=False)
x = tf.keras.layers.GlobalAveragePooling2D()(x)
x = tf.keras.layers.Dropout(0.2)(x)
outputs = tf.keras.layers.Dense(10, activation='softmax')(x)
model = tf.keras.Model(inputs, outputs)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics=["accuracy"])

"""**Check performance before training.**"""

loss0, accuracy0 = model.evaluate(validation_dataset)
print("Initial validation loss:", loss0)
print("Initial validation accuracy:", accuracy0)

"""**Training callbacks.**"""

from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping

reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)
checkpoint = ModelCheckpoint('best_tuned_model.keras', monitor='val_loss', save_best_only=True, verbose=1)
early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)

"""**Initial training.**"""

initial_epochs = 30

history = model.fit(
    train_dataset,
    epochs=initial_epochs,
    validation_data=validation_dataset,
    callbacks=[reduce_lr, checkpoint, early_stop]
)

"""**Evaluate on test set.**"""

loss1, accuracy1 = model.evaluate(test_dataset)
print("Test loss:", loss1)
print("Test accuracy:", accuracy1)

"""**Plot accuracy and loss.**"""

history_df = pd.DataFrame(history.history)

plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
plt.plot(history_df['accuracy'], label='Train Acc')
plt.plot(history_df['val_accuracy'], label='Val Acc')
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.title("Training vs Validation Accuracy")

plt.subplot(1, 2, 2)
plt.plot(history_df['loss'], label='Train Loss')
plt.plot(history_df['val_loss'], label='Val Loss')
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.title("Training vs Validation Loss")

plt.show()

"""**Confusion Matrix.**"""

from sklearn.metrics import confusion_matrix

y_true, y_pred = [], []
for images, labels in test_dataset:
    preds = model.predict(images, verbose=0)
    y_true.extend(labels.numpy())
    y_pred.extend(np.argmax(preds, axis=1))

cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_names, yticklabels=class_names)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

from sklearn.metrics import classification_report

print(classification_report(y_true, y_pred, target_names=class_names))

"""##Fine-Tuning Step.

**Fine-tune top layers**
"""

base_model.trainable = True
fine_tune_at = 100  # freeze layers before this index
for layer in base_model.layers[:fine_tune_at]:
    layer.trainable = False

# Compile again with a low learning rate
model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics=['accuracy'])

# Continue training
fine_tune_epochs = 10
total_epochs = initial_epochs + fine_tune_epochs

history_fine = model.fit(
    train_dataset,
    epochs=total_epochs,
    initial_epoch=history.epoch[-1],
    validation_data=validation_dataset,
    callbacks=[reduce_lr, checkpoint, early_stop]
)

